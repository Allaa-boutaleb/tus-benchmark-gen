{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b425bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#load the pickle file as a dictionary\n",
    "def loadDictionaryFromPickleFile(dictionaryPath):\n",
    "    print(\"Loading dictionary at:\", dictionaryPath)\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    print(\"The total number of keys in the dictionary are:\", len(dictionary))\n",
    "    return dictionary\n",
    "\n",
    "#This function saves dictionaries as pickle files in the storage.\n",
    "def saveDictionaryAsPickleFile(dictionary, dictionaryPath):\n",
    "    filePointer=open(dictionaryPath, 'wb')\n",
    "    pickle.dump(dictionary,filePointer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    filePointer.close()\n",
    "\n",
    "def loadDictionaryFromPickleFileList(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    actual_dict = {}\n",
    "    for curr_dict in dictionary:\n",
    "        actual_dict[curr_dict['query_table']] = curr_dict['result_set']\n",
    "    return actual_dict\n",
    "\n",
    "which_benchmark_num = 0\n",
    "while(which_benchmark_num!= 1 and which_benchmark_num != 2 and which_benchmark_num != 3 and which_benchmark_num != 4):\n",
    "    #print(\"which benchmark? Press 1 for TUS, 2 for Labelled\")\n",
    "    print(\"which benchmark? Press 1 for TUS-Small, 2 for SANTOS-Small, 3 for UGEN_V1, 4 for UGEN_V2\")\n",
    "    which_benchmark_num = int(input())\n",
    "if which_benchmark_num == 1:\n",
    "    #all_methods = [\"d3l\", 'turl', 'santos_full', 'santos_kb', 'santos_synth']\n",
    "    all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "    which_benchmark = \"tus\"\n",
    "    max_k = 60\n",
    "    k_range = 5\n",
    "elif which_benchmark_num == 2: #which_benchmark == 2\n",
    "    #all_methods = [\"d3l\", 'santos_full', 'santos_kb', 'santos_synth']\n",
    "    all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "    which_benchmark = \"santos\"\n",
    "    max_k = 10\n",
    "    k_range = 1\n",
    "elif which_benchmark_num == 3:\n",
    "    all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "    which_benchmark = \"ugen_v1\"\n",
    "    max_k = 10\n",
    "    k_range = 1\n",
    "else:\n",
    "    all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "    which_benchmark = \"ugen_v2\"\n",
    "    max_k = 10\n",
    "    k_range = 1\n",
    "\n",
    "col_number = 3 \n",
    "\n",
    "colors = {\"d3l\":\"#e52638\",\n",
    "              \"santos\": \"#68affc\",\n",
    "              \"starmie\":\"#699f3c\",\n",
    "              \"starmie_vicuna_zero\": \"#b04b3b\",\n",
    "              \"starmie_vicuna_opt\": \"#ffa600\"\n",
    "              }\n",
    "linestyles = {\"d3l\":\"dashed\",\n",
    "              \"santos\": \"solid\",\n",
    "              \"starmie\":\"dotted\",\n",
    "              \"starmie_vicuna_zero\": \"dashdot\",\n",
    "              \"starmie_vicuna_opt\": (0, (3, 1, 1, 1, 1, 1))\n",
    "              }\n",
    "\n",
    "labels = {\"d3l\":r\"$D^{3}L$\",\n",
    "#           \"turl\": r\"TURL\",\n",
    "          \"santos\": r\"$SANTOS$\",\n",
    "          \"starmie\":r\"$Starmie$\",\n",
    "          \"starmie_vicuna_zero\": r\"$Starmie-Vicuna_{Zero}$\",\n",
    "          \"starmie_vicuna_opt\": r\"$Starmie-Vicuna_{Optim}$\"\n",
    "          }\n",
    "\n",
    "markers = {\"d3l\":\"^\",\n",
    "           \"santos\": \"s\",\n",
    "           \"starmie\":\"o\",\n",
    "           \"starmie_vicuna_zero\": \"*\",\n",
    "           \"starmie_vicuna_opt\": \"+\"\n",
    "        }\n",
    "\n",
    "precision_list_dict = {}\n",
    "recall_list_dict = {}\n",
    "\n",
    "query_mean_precision_list_dict = {}\n",
    "\n",
    "total_ground_truth_size = 0\n",
    "used_queries = 0\n",
    "groundtruth = loadDictionaryFromPickleFile(r\"groundtruth/\"+which_benchmark+\"UnionBenchmark.pickle\")\n",
    "#groundtruth = loadDictionaryFromPickleFile(r\"groundtruth/validated_ugenv1_gt.pickle\")\n",
    "\n",
    "for table in groundtruth:\n",
    "    #t28 tables have less than 60 results. So, skipping them in the analysis.\n",
    "    #skipping t28, we have 125 query tables.\n",
    "    if table.split(\"____\",1)[0] != \"t_28dc8f7610402ea7\": \n",
    "        used_queries += 1\n",
    "        total_ground_truth_size += len(groundtruth[table])\n",
    "total_ground_truth_size = total_ground_truth_size / used_queries\n",
    "while(max_k % k_range != 0):\n",
    "    print(\"Enter k for mean average precision. 5 should be its factor:\")\n",
    "    max_k = int(input())\n",
    "for which_method in all_methods:\n",
    "    resPath = r\"new_stats/\"+which_benchmark+\"_benchmark_result_by_\"+which_method+\".pickle\"\n",
    "    resultFile = loadDictionaryFromPickleFile(resPath)\n",
    "    if type(resultFile) == list:\n",
    "        resultFile = loadDictionaryFromPickleFileList(resPath)\n",
    "    \n",
    "    all_query_results = {}\n",
    "    \n",
    "    \n",
    "    # =============================================================================\n",
    "    # Test code for testing how many tp is received for each query table\n",
    "    # =============================================================================\n",
    "    each_tp_s = {}\n",
    "    for table in resultFile:\n",
    "        groundtruth_set = set(groundtruth[table])\n",
    "        result_set = resultFile[table][:len(groundtruth_set)]\n",
    "        find_intersection = set(result_set).intersection(groundtruth_set)\n",
    "        each_tp_s[table] = (len(groundtruth_set), len(find_intersection))\n",
    "        \n",
    "    # =============================================================================\n",
    "    # Precision and recall starts here\n",
    "    # =============================================================================\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    per_table_map_stats_precision = {}\n",
    "    per_table_map_stats_recall = {}\n",
    "    for k in range(1, max_k+1):\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        rec = 0\n",
    "        for table in resultFile:\n",
    "            if table.split(\"____\",1)[0] != \"t_28dc8f7610402ea7\": \n",
    "                groundtruth_set = set(groundtruth[table])\n",
    "                groundtruth_set = {x.split(\".\")[0] for x in groundtruth_set}\n",
    "                result_set = resultFile[table][:k]\n",
    "                result_set = [x.split(\".\")[0] for x in result_set]\n",
    "                find_intersection = set(result_set).intersection(groundtruth_set)\n",
    "                tp = len(find_intersection)\n",
    "                #fp = k - tp\n",
    "                if which_method == \"starmie_gpt3\" and k==10:\n",
    "                    print(\"LENGTH OF RESULT SET\", len(result_set))\n",
    "                fp = k - tp\n",
    "                fn = len(groundtruth_set) - tp\n",
    "                if len(groundtruth_set)>=k: \n",
    "                    true_positive += tp\n",
    "                    false_positive += fp\n",
    "                    false_negative += fn\n",
    "                query_table_name = table.split(\".\")[0].split('_')[0]\n",
    "                curr_precision = 0.0\n",
    "                if tp != 0 or fp != 0:\n",
    "                    curr_precision = tp / (tp + fp)\n",
    "                curr_recall = 0.0\n",
    "                if tp != 0 or fn != 0:\n",
    "                    curr_recall = tp / (tp+fn)\n",
    "                if query_table_name not in per_table_map_stats_precision:\n",
    "                    per_table_map_stats_precision[query_table_name] = [curr_precision]\n",
    "                    per_table_map_stats_recall[query_table_name] = [curr_recall]\n",
    "                else:\n",
    "                    per_table_map_stats_precision[query_table_name].append(curr_precision)\n",
    "                    per_table_map_stats_recall[query_table_name].append(curr_recall)\n",
    "                rec += tp / (tp+fn)\n",
    "                    \n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = rec/len(resultFile)\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "    \n",
    "    used_k = [k_range]\n",
    "    if max_k >k_range:\n",
    "        for i in range(k_range * 2, max_k+1, k_range):\n",
    "            used_k.append(i)\n",
    "    print(\"--------------------------\")\n",
    "    for k in used_k:\n",
    "        print(\"Precision at k = \",k,\"=\", precision_array[k-1])\n",
    "        if which_method not in precision_list_dict:\n",
    "            precision_list_dict[which_method] = [precision_array[k-1]]\n",
    "        else:\n",
    "            precision_list_dict[which_method].append(precision_array[k-1])\n",
    "        \n",
    "        print(\"Recall at k = \",k,\"=\", recall_array[k-1])\n",
    "        if which_method not in recall_list_dict:\n",
    "            recall_list_dict[which_method] = [recall_array[k-1]]\n",
    "        else:\n",
    "            recall_list_dict[which_method].append(recall_array[k-1])\n",
    "        print(\"--------------------------\")\n",
    "#         for key, value in per_table_map_stats_precision.items():\n",
    "#             print(\"QUERY TABLE\", key)\n",
    "#             print(\"Precision at query k = \",k,\"=\", value[k-1])\n",
    "#             print(\"Recall at query k = \",k,\"=\", per_table_map_stats_recall[key][k-1])\n",
    "#         print(\"--------------------------\")\n",
    "    map_sum = 0\n",
    "    for k in range(0, max_k):\n",
    "        map_sum += precision_array[k]\n",
    "    mean_avg_pr = map_sum/max_k\n",
    "    print(\"The mean average precision is:\", mean_avg_pr)\n",
    "    \n",
    "    if which_method not in query_mean_precision_list_dict and which_benchmark == \"ugen_v2\":\n",
    "        map_values = []\n",
    "        \n",
    "        for key, value in per_table_map_stats_precision.items():\n",
    "            map_sum_query = 0\n",
    "            for k in range(0, max_k):\n",
    "                map_sum_query += value[k]\n",
    "            mean_avg_pr_query = map_sum_query/max_k\n",
    "            map_values.append((key,mean_avg_pr_query))\n",
    "        \n",
    "        query_mean_precision_list_dict[which_method] = map_values\n",
    "        # Sort the list based on the 1-indexed values in descending order\n",
    "        sorted_data = sorted(map_values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the top 5 highest values\n",
    "        top_5_high_values = sorted_data[:5]\n",
    "        bottom_5_values = sorted_data[-5:]\n",
    "\n",
    "        print(\"TOP 5 HIGH VALUES\", top_5_high_values)\n",
    "        print(\"BOTTOM 5 VALUES\", bottom_5_values)\n",
    "        \n",
    "#k = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "k = used_k\n",
    "count = 0\n",
    "fsize = 18\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "#ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "#plt.subplot(1, 2, 1)\n",
    "plt.xticks(np.arange(0, max_k + k_range, k_range), fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "for which_method in precision_list_dict:\n",
    "    plt.plot(k, precision_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "#plt.title(\"Precision\")\n",
    "plt.xlabel(\"k\", fontsize=fsize)\n",
    "plt.ylabel(\"P@k\", fontsize=fsize)\n",
    "#plt.xticks([0, 10, 2])\n",
    "#plt.figtext(0.5, 0.01, \"(a) Average Precision in TUS benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "#plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_in_'+which_benchmark+'.pdf')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(6,5))\n",
    "#plt.subplot(1, 2,2)\n",
    "plt.xticks(np.arange(0, max_k + k_range, k_range), fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "y_axis = []\n",
    "for val in k:\n",
    "    y_axis.append(val/total_ground_truth_size)\n",
    "\n",
    "count = 0\n",
    "for which_method in recall_list_dict:\n",
    "    plt.plot(k, recall_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "\n",
    "#plt.title(\"Precision\")\n",
    "\n",
    "plt.xlabel(\"k\", fontsize=fsize)\n",
    "plt.ylabel(\"R@k\", fontsize=fsize)\n",
    "#plt.legend()\n",
    "#plt.legend(ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, 1.15), borderaxespad=-2, frameon = False, fontsize = fsize)\n",
    "#plt.legend(ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, 1.11), borderaxespad=-0.5, frameon = False, fontsize = fsize)\n",
    "#plt.figtext(0.5, 0.01, \"(b) Average Recall in TUS Benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "#plt.subplot(2, 2,3, colspan = 2)\n",
    "#plt.legend(ncol=5, loc=\"upper center\", bbox_to_anchor=(0, 0, 1, 1), frameon = False, fontsize = fsize)\n",
    "#k /total_ground_truth_size\n",
    "plt.plot(k, y_axis, color = \"black\", label = \"IDEAL-Recall\", linewidth = 2)\n",
    "#plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.ylim(top=0.6)\n",
    "plt.savefig('recall_in_'+which_benchmark+'.pdf')  \n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(14,5))\n",
    "#plt.subplot(1, 2,2)\n",
    "plt.xticks(np.arange(0, max_k + k_range, k_range), fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "y_axis = []\n",
    "for val in k:\n",
    "    y_axis.append(val/total_ground_truth_size)\n",
    "\n",
    "count = 0\n",
    "for which_method in recall_list_dict:\n",
    "    plt.plot(k, recall_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "\n",
    "#plt.title(\"Precision\")\n",
    "\n",
    "plt.xlabel(\"k\", fontsize=fsize)\n",
    "plt.ylabel(\"R@k\", fontsize=fsize)\n",
    "#plt.legend()\n",
    "#plt.legend(ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, 1.15), borderaxespad=-2, frameon = False, fontsize = fsize)\n",
    "#plt.legend(ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, 1.11), borderaxespad=-0.5, frameon = False, fontsize = fsize)\n",
    "#plt.figtext(0.5, 0.01, \"(b) Average Recall in TUS Benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "#plt.subplot(2, 2,3, colspan = 2)\n",
    "#plt.legend(ncol=5, loc=\"upper center\", bbox_to_anchor=(0, 0, 1, 1), frameon = False, fontsize = fsize)\n",
    "#k /total_ground_truth_size\n",
    "plt.plot(k, y_axis, color = \"black\", label = \"IDEAL-Recall\", linewidth = 2)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.5), borderaxespad=0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('legend_'+which_benchmark+'.pdf', dpi=400)  \n",
    "plt.show()\n",
    "\n",
    "if which_benchmark == \"ugen_v2\":\n",
    "    for which_method in all_methods:\n",
    "        plt.clf()\n",
    "\n",
    "        #plt.figure(figsize=(15, 5))\n",
    "        # Separate x and y values from the list of tuples\n",
    "        x_values, y_values = zip(*query_mean_precision_list_dict[which_method])\n",
    "        x_ticks_positions = range(len(x_values))\n",
    "\n",
    "        # Calculate the width of the bars (distance between consecutive x-tick positions)\n",
    "    #     bar_width = 0.6\n",
    "    #     # Calculate the appropriate figure size based on the number of bars\n",
    "    #     figure_width = 50 * bar_width\n",
    "    #     figure_height = 6  # You can adjust this value based on your preference\n",
    "        print(\"Which method\", which_method)\n",
    "        # Set the appropriate figure size\n",
    "        \n",
    "        plt.figure(figsize=(2.5, 15))\n",
    "        #plt.xticks(rotation='vertical')\n",
    "        plt.rcParams.update({'font.size': 14})\n",
    "        # Create the bar plot with equal spacing between bars\n",
    "        #plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.5, fancybox=False, shadow=False)\n",
    "        plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "        plt.barh(x_ticks_positions, y_values)\n",
    "        # Set the x-tick positions and labels\n",
    "        \n",
    "        if which_method == 'd3l':\n",
    "            # Add labels and title\n",
    "            plt.yticks(x_ticks_positions, x_values)\n",
    "            plt.ylabel('Topics')\n",
    "            plt.xlabel('MAP@k')\n",
    "        else:\n",
    "            plt.xlabel('MAP@k')\n",
    "            plt.yticks(x_ticks_positions, '')\n",
    "        # Set the y-axis limits\n",
    "        plt.xlim(0, 1)  # Add some padding for visualization\n",
    "        #plt.subplots_adjust(left=0, right=1, top=0.95, bottom=0.1)\n",
    "        # Show the plot\n",
    "        plt.margins(0.02)\n",
    "        #plt.show()\n",
    "        plt.savefig(f'legend_{which_method}_+query_specific.pdf', bbox_inches=\"tight\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e9629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### CODE FOR CONFUSION MATRIX\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "#load the pickle file as a dictionary\n",
    "def loadDictionaryFromPickleFile(dictionaryPath):\n",
    "    print(\"Loading dictionary at:\", dictionaryPath)\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    print(\"The total number of keys in the dictionary are:\", len(dictionary))\n",
    "    return dictionary\n",
    "\n",
    "#This function saves dictionaries as pickle files in the storage.\n",
    "def saveDictionaryAsPickleFile(dictionary, dictionaryPath):\n",
    "    filePointer=open(dictionaryPath, 'wb')\n",
    "    pickle.dump(dictionary,filePointer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    filePointer.close()\n",
    "\n",
    "def loadDictionaryFromPickleFileList(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    actual_dict = {}\n",
    "    for curr_dict in dictionary:\n",
    "        actual_dict[curr_dict['query_table']] = curr_dict['result_set']\n",
    "    return actual_dict\n",
    "\n",
    "which_benchmark_num = 3\n",
    "all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "which_benchmark = \"ugen_v2\"\n",
    "max_k = 10\n",
    "k_range = 1\n",
    "\n",
    "col_number = 3 \n",
    "\n",
    "colors = {\"d3l\":\"#e52638\",\n",
    "              \"santos\": \"#68affc\",\n",
    "              \"starmie\":\"#699f3c\",\n",
    "              \"starmie_vicuna_zero\": \"#b04b3b\",\n",
    "              \"starmie_vicuna_opt\": \"#ffa600\"\n",
    "              }\n",
    "linestyles = {\"d3l\":\"dashed\",\n",
    "              \"santos\": \"solid\",\n",
    "              \"starmie\":\"dotted\",\n",
    "              \"starmie_vicuna_zero\": \"dashdot\",\n",
    "              \"starmie_vicuna_opt\": (0, (3, 1, 1, 1, 1, 1))\n",
    "              }\n",
    "\n",
    "labels = {\"d3l\":r\"$D^{3}L$\",\n",
    "#           \"turl\": r\"TURL\",\n",
    "          \"santos\": r\"$SANTOS$\",\n",
    "          \"starmie\":r\"$Starmie$\",\n",
    "          \"starmie_vicuna_zero\": r\"$Starmie-Vicuna_{Zero}$\",\n",
    "          \"starmie_vicuna_opt\": r\"$Starmie-Vicuna_{Optim}$\"\n",
    "          }\n",
    "\n",
    "markers = {\"d3l\":\"^\",\n",
    "           \"santos\": \"s\",\n",
    "           \"starmie\":\"o\",\n",
    "           \"starmie_vicuna_zero\": \"*\",\n",
    "           \"starmie_vicuna_opt\": \"+\"\n",
    "        }\n",
    "\n",
    "precision_list_dict = {}\n",
    "recall_list_dict = {}\n",
    "\n",
    "query_mean_precision_list_dict = {}\n",
    "\n",
    "total_ground_truth_size = 0\n",
    "used_queries = 0\n",
    "groundtruth = pd.read_csv(r\"groundtruth/\"+which_benchmark+\"_groundtruth.csv\")\n",
    "\n",
    "tp_list = []\n",
    "fp_list = []\n",
    "tn_list = []\n",
    "fn_list = []\n",
    "\n",
    "for which_method in all_methods:\n",
    "    curr_tp = 0\n",
    "    curr_fp = 0\n",
    "    curr_tn = 0\n",
    "    curr_fn = 0\n",
    "    resPath = r\"new_stats/\"+which_benchmark+\"_benchmark_result_by_\"+which_method+\".pickle\"\n",
    "    resultFile = loadDictionaryFromPickleFile(resPath)\n",
    "    if type(resultFile) == list:\n",
    "        resultFile = loadDictionaryFromPickleFileList(resPath)\n",
    "\n",
    "    all_query_results = {}\n",
    "\n",
    "    for _,row in groundtruth.iterrows():\n",
    "        curr_query_table = row['query_table']\n",
    "        curr_data_lake_table = row['data_lake_table']\n",
    "        is_unionable = True if int(row['unionable']) == 1 else False\n",
    "        result_unionable = False\n",
    "        curr_result_set = resultFile[curr_query_table]\n",
    "        if len(curr_result_set) > 1:\n",
    "            if not curr_result_set[0].endswith('.csv'):\n",
    "                curr_data_lake_table = curr_data_lake_table.split('.')[0]\n",
    "        if curr_data_lake_table in curr_result_set:\n",
    "            result_unionable = True\n",
    "        if result_unionable == is_unionable:\n",
    "            if is_unionable:\n",
    "                curr_tp += 1\n",
    "            else:\n",
    "                curr_tn += 1\n",
    "        else:\n",
    "            if is_unionable:\n",
    "                curr_fn += 1\n",
    "            else:\n",
    "                curr_fp += 1\n",
    "    print(f\"Currently at {which_method}\")\n",
    "    accuracy = float(curr_tp + curr_tn) / (curr_tp + curr_tn + curr_fp + curr_fn)\n",
    "    print(\"Accuracy\", accuracy)\n",
    "    corner_case = float(curr_fp + curr_fn) / (curr_tp + curr_tn + curr_fp + curr_fn)\n",
    "    print(\"Corner Case Ratio\", corner_case)    \n",
    "    plt.clf()\n",
    "    cm = np.array([[curr_tp, curr_fn],\n",
    "                   [curr_fp, curr_tn]])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=sns.light_palette(\"navy\", 12), xticklabels=['Unionable', 'Non-Unionable'], yticklabels=['Unionable', 'Non-Unionable'], vmin=50, vmax=450)\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'cm_{which_method}.pdf')\n",
    "    plt.show()\n",
    "    #plot_confusion_matrix(curr_tp, curr_fp, curr_tn, curr_fn)\n",
    "\n",
    "#     print(curr_fp)\n",
    "#     print(curr_tn)\n",
    "#     print(curr_fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c414cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#load the pickle file as a dictionary\n",
    "def loadDictionaryFromPickleFile(dictionaryPath):\n",
    "    print(\"Loading dictionary at:\", dictionaryPath)\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    print(\"The total number of keys in the dictionary are:\", len(dictionary))\n",
    "    return dictionary\n",
    "\n",
    "#This function saves dictionaries as pickle files in the storage.\n",
    "def saveDictionaryAsPickleFile(dictionary, dictionaryPath):\n",
    "    filePointer=open(dictionaryPath, 'wb')\n",
    "    pickle.dump(dictionary,filePointer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    filePointer.close()\n",
    "\n",
    "def loadDictionaryFromPickleFileList(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = pickle.load(filePointer)\n",
    "    filePointer.close()\n",
    "    actual_dict = {}\n",
    "    for curr_dict in dictionary:\n",
    "        actual_dict[curr_dict['query_table']] = curr_dict['result_set']\n",
    "    return actual_dict\n",
    "\n",
    "which_ablation_num = 0\n",
    "which_ablation = \"\"\n",
    "while(which_ablation_num!= 1 and which_ablation_num != 2):\n",
    "    #print(\"which benchmark? Press 1 for TUS, 2 for Labelled\")\n",
    "    print(\"which ablation? Press 1 for ICL, 2 for Sparsity\")\n",
    "    which_ablation_num = int(input())\n",
    "if which_ablation_num == 1:\n",
    "    #all_methods = [\"d3l\", 'turl', 'santos_full', 'santos_kb', 'santos_synth']\n",
    "    all_methods = [\"starmie_gpt2xl\",'starmie_alpaca','starmie_vicuna']\n",
    "    which_benchmark = \"ugen_v2\"\n",
    "    which_ablation = \"icl\"\n",
    "    max_k = 10\n",
    "    k_range = 1\n",
    "else: #which_benchmark == 2\n",
    "    #all_methods = [\"d3l\", 'santos_full', 'santos_kb', 'santos_synth']\n",
    "    all_methods = [\"d3l\", 'santos', 'starmie','starmie_vicuna_zero', 'starmie_vicuna_opt']\n",
    "    which_benchmark = \"ugen_v2\"\n",
    "    which_ablation = \"sparse\"\n",
    "    max_k = 10\n",
    "    k_range = 1\n",
    "\n",
    "col_number = 3 \n",
    "\n",
    "colors = {\"d3l\":\"#e52638\",\n",
    "              \"santos\": \"#68affc\",\n",
    "              \"starmie\":\"#699f3c\",\n",
    "              \"starmie_vicuna_zero\": \"#b04b3b\",\n",
    "              \"starmie_vicuna_opt\": \"#ffa600\"\n",
    "              }\n",
    "if which_ablation_num == 1:\n",
    "    colors = {\"starmie_gpt2xl\": \"#b04b3b\",\n",
    "              \"starmie_gpt3\": \"#68affc\",\n",
    "              \"starmie_alpaca\":\"#699f3c\",\n",
    "              \"starmie_vicuna\":\"#e52638\",\n",
    "              }\n",
    "    \n",
    "    \n",
    "linestyles = {\"d3l\":\"dashed\",\n",
    "              \"santos\": \"solid\",\n",
    "              \"starmie\":\"dotted\",\n",
    "              \"starmie_vicuna_zero\": \"dashdot\",\n",
    "              \"starmie_vicuna_opt\": (0, (3, 1, 1, 1, 1, 1))\n",
    "              }\n",
    "if which_ablation_num == 1:\n",
    "    linestyles = {\"starmie_gpt2xl\":\"dashed\",\n",
    "                  \"starmie_gpt3\": \"solid\",\n",
    "                  \"starmie_alpaca\":\"dotted\",\n",
    "                  \"starmie_vicuna\": \"dashdot\"\n",
    "                  }\n",
    "\n",
    "labels = {\"d3l\":r\"$D^{3}L$\",\n",
    "#           \"turl\": r\"TURL\",\n",
    "          \"santos\": r\"$SANTOS$\",\n",
    "          \"starmie\":r\"$Starmie$\",\n",
    "          \"starmie_vicuna_zero\": r\"$Starmie-Vicuna_{Zero}$\",\n",
    "          \"starmie_vicuna_opt\": r\"$Starmie-Vicuna_{Optim}$\"\n",
    "          }\n",
    "if which_ablation_num == 1:\n",
    "    labels = {\"starmie_gpt2xl\":r\"Starmie-GPT2XL\",\n",
    "              \"starmie_gpt3\": r\"Starmie-GPT3\",\n",
    "              \"starmie_alpaca\":r\"Starmie-Alpaca\",\n",
    "              \"starmie_vicuna\": r\"Starmie-Vicuna\"\n",
    "              }    \n",
    "\n",
    "markers = {\"d3l\":\"^\",\n",
    "           \"santos\": \"s\",\n",
    "           \"starmie\":\"o\",\n",
    "           \"starmie_vicuna_zero\": \"*\",\n",
    "           \"starmie_vicuna_opt\": \"+\"\n",
    "        }\n",
    "if which_ablation_num == 1:\n",
    "    markers = {\"starmie_gpt2xl\":\"^\",\n",
    "               \"starmie_gpt3\": \"s\",\n",
    "               \"starmie_alpaca\":\"o\",\n",
    "               \"starmie_vicuna\": \"*\"\n",
    "            }\n",
    "    \n",
    "precision_list_dict = {}\n",
    "recall_list_dict = {}\n",
    "mean_precision_list_dict = {}\n",
    "ablation_val_list = []\n",
    "total_ground_truth_size = 0\n",
    "used_queries = 0\n",
    "groundtruth = loadDictionaryFromPickleFile(r\"groundtruth/\"+which_benchmark+\"UnionBenchmark.pickle\")\n",
    "\n",
    "for table in groundtruth:\n",
    "    #t28 tables have less than 60 results. So, skipping them in the analysis.\n",
    "    #skipping t28, we have 125 query tables.\n",
    "    if table.split(\"____\",1)[0] != \"t_28dc8f7610402ea7\": \n",
    "        used_queries += 1\n",
    "        total_ground_truth_size += len(groundtruth[table])\n",
    "total_ground_truth_size = total_ground_truth_size / used_queries\n",
    "while(max_k % k_range != 0):\n",
    "    print(\"Enter k for mean average precision. 5 should be its factor:\")\n",
    "    max_k = int(input())\n",
    "\n",
    "if which_ablation == \"icl\":\n",
    "    ablation_val_list = [0,1,2,3]\n",
    "else:\n",
    "    ablation_val_list = [0,5,10,15,20]\n",
    "for which_method in all_methods:\n",
    "    for curr_ablation in ablation_val_list:\n",
    "        resPath = f\"new_stats/{which_benchmark}_benchmark_result_by_{which_method}_{which_ablation}_{curr_ablation}.pickle\"\n",
    "        resultFile = loadDictionaryFromPickleFile(resPath)\n",
    "        if type(resultFile) == list:\n",
    "            resultFile = loadDictionaryFromPickleFileList(resPath)\n",
    "        all_query_results = {}\n",
    "\n",
    "\n",
    "        # =============================================================================\n",
    "        # Test code for testing how many tp is received for each query table\n",
    "        # =============================================================================\n",
    "        each_tp_s = {}\n",
    "        for table in resultFile:\n",
    "            groundtruth_set = set(groundtruth[table])\n",
    "            result_set = resultFile[table][:len(groundtruth_set)]\n",
    "            find_intersection = set(result_set).intersection(groundtruth_set)\n",
    "            each_tp_s[table] = (len(groundtruth_set), len(find_intersection))\n",
    "\n",
    "        # =============================================================================\n",
    "        # Precision and recall starts here\n",
    "        # =============================================================================\n",
    "        precision_array = []\n",
    "        recall_array = []\n",
    "        for k in range(1, max_k+1):\n",
    "            true_positive = 0\n",
    "            false_positive = 0\n",
    "            false_negative = 0\n",
    "            rec = 0\n",
    "            for table in resultFile:\n",
    "                if table.split(\"____\",1)[0] != \"t_28dc8f7610402ea7\": \n",
    "                    groundtruth_set = set(groundtruth[table])\n",
    "                    groundtruth_set = {x.split(\".\")[0] for x in groundtruth_set}\n",
    "                    result_set = resultFile[table][:k]\n",
    "                    result_set = [x.split(\".\")[0] for x in result_set]\n",
    "                    find_intersection = set(result_set).intersection(groundtruth_set)\n",
    "                    tp = len(find_intersection)\n",
    "                    fp = k - tp\n",
    "                    fn = len(groundtruth_set) - tp\n",
    "                    if len(groundtruth_set)>=k: \n",
    "                        true_positive += tp\n",
    "                        false_positive += fp\n",
    "                        false_negative += fn\n",
    "                    query_table_name = table.split(\".\")[0].split('_')[0]\n",
    "                    curr_precision = 0.0\n",
    "                    if tp != 0 or fp != 0:\n",
    "                        curr_precision = tp / (tp + fp)\n",
    "                    curr_recall = 0.0\n",
    "                    if tp != 0 or fn != 0:\n",
    "                        curr_recall = tp / (tp+fn)\n",
    "                    rec += tp / (tp+fn)\n",
    "\n",
    "            precision = true_positive / (true_positive + false_positive)\n",
    "            recall = rec/len(resultFile)\n",
    "            precision_array.append(precision)\n",
    "            recall_array.append(recall)\n",
    "\n",
    "        used_k = [k_range]\n",
    "        if max_k >k_range:\n",
    "            for i in range(k_range * 2, max_k+1, k_range):\n",
    "                used_k.append(i)\n",
    "        print(\"--------------------------\")\n",
    "        print(\"Precision at k = \",max_k,\"=\", precision_array[max_k-1])\n",
    "        if which_method not in precision_list_dict:\n",
    "            precision_list_dict[which_method] = [precision_array[max_k-1]]\n",
    "        else:\n",
    "            precision_list_dict[which_method].append(precision_array[k-1])\n",
    "\n",
    "        print(\"Recall at k = \",max_k,\"=\", recall_array[max_k-1])\n",
    "        if which_method not in recall_list_dict:\n",
    "            recall_list_dict[which_method] = [recall_array[max_k-1]]\n",
    "        else:\n",
    "            recall_list_dict[which_method].append(recall_array[max_k-1])\n",
    "        print(\"--------------------------\")\n",
    "    #         for key, value in per_table_map_stats_precision.items():\n",
    "    #             print(\"QUERY TABLE\", key)\n",
    "    #             print(\"Precision at query k = \",k,\"=\", value[k-1])\n",
    "    #             print(\"Recall at query k = \",k,\"=\", per_table_map_stats_recall[key][k-1])\n",
    "    #         print(\"--------------------------\")\n",
    "        map_sum = 0\n",
    "        for k in range(0, max_k):\n",
    "            map_sum += precision_array[k]\n",
    "        mean_avg_pr = map_sum/max_k\n",
    "        print(\"MAP\", mean_avg_pr)\n",
    "        if which_method not in mean_precision_list_dict:\n",
    "            mean_precision_list_dict[which_method] = [mean_avg_pr]\n",
    "        else:\n",
    "            mean_precision_list_dict[which_method].append(mean_avg_pr)\n",
    "        \n",
    "\n",
    "k = ablation_val_list\n",
    "count = 0\n",
    "fsize = 24\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "#ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "#plt.subplot(1, 2, 1)\n",
    "\n",
    "k_range = 1 if which_ablation == \"icl\" else 5\n",
    "plt.xticks(ablation_val_list, fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "for which_method in precision_list_dict:\n",
    "    print(precision_list_dict[which_method])\n",
    "    plt.plot(k, precision_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "#plt.title(\"Precision\")\n",
    "xlabel = \"\"\n",
    "if which_ablation == \"icl\":\n",
    "    xlabel = \"# ICL Examples\"\n",
    "else:\n",
    "    xlabel = \"Sparsity % Added\"\n",
    "plt.xlabel(xlabel, fontsize=fsize)\n",
    "plt.ylabel(\"P@10\", fontsize=fsize)\n",
    "#plt.xticks([0, 10, 2])\n",
    "#plt.figtext(0.5, 0.01, \"(a) Average Precision in TUS benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "#plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'precision_{which_ablation}_in_'+which_benchmark+'.pdf')\n",
    "#plt.ylim(0.1,)\n",
    "#plt.show()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.xticks(ablation_val_list, fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "for which_method in mean_precision_list_dict:\n",
    "    plt.plot(k, mean_precision_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "#plt.title(\"Precision\")\n",
    "xlabel = \"\"\n",
    "if which_ablation == \"icl\":\n",
    "    xlabel = \"# ICL Examples\"\n",
    "else:\n",
    "    xlabel = \"Sparsity % Added\"\n",
    "plt.xlabel(xlabel, fontsize=fsize)\n",
    "plt.ylabel(\"MAP@10\", fontsize=fsize)\n",
    "#plt.ylim(0.1,)\n",
    "#plt.xticks([0, 10, 2])\n",
    "#plt.figtext(0.5, 0.01, \"(a) Average Precision in TUS benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'legend_{which_ablation}_in_'+which_benchmark+'.pdf')\n",
    "#plt.show()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.xticks(ablation_val_list, fontsize=fsize)\n",
    "plt.yticks(fontsize=fsize)\n",
    "for which_method in mean_precision_list_dict:\n",
    "    plt.plot(k, mean_precision_list_dict[which_method], color = colors[which_method], linestyle = linestyles[which_method], label = labels[which_method], marker=markers[which_method])\n",
    "    count += 1\n",
    "#plt.title(\"Precision\")\n",
    "xlabel = \"\"\n",
    "if which_ablation == \"icl\":\n",
    "    xlabel = \"# ICL Examples\"\n",
    "else:\n",
    "    xlabel = \"Sparsity % Added\"\n",
    "plt.xlabel(xlabel, fontsize=fsize)\n",
    "plt.ylabel(\"MAP@10\", fontsize=fsize)\n",
    "#plt.ylim(0.1,)\n",
    "#plt.xticks([0, 10, 2])\n",
    "#plt.figtext(0.5, 0.01, \"(a) Average Precision in TUS benchmark\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "#plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), borderaxespad=-0.05, fancybox=False, shadow=False, ncol=col_number, fontsize = fsize)\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'MAP_{which_ablation}_in_'+which_benchmark+'.pdf')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58690eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fe4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
