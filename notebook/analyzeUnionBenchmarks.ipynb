{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from pandas.errors import EmptyDataError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9926777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datalake_and_query_folders(in_dir, out_dir, gtfile, query_col=\"query_table\", datalake_col=\"data_lake_table\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    query_folder = os.path.join(out_dir, \"query\")\n",
    "    datalake_folder = os.path.join(out_dir, \"datalake\")\n",
    "    os.makedirs(query_folder, exist_ok=True)\n",
    "    os.makedirs(datalake_folder, exist_ok=True)\n",
    "    gt_df = pd.read_csv(gtfile)\n",
    "    counter = 0\n",
    "    for _, row in gt_df.iterrows():\n",
    "        query_table = row[query_col]\n",
    "        datalake_table = row[datalake_col]\n",
    "        qt_file = os.path.join(query_folder, query_table)\n",
    "        dlt_file = os.path.join(datalake_folder, datalake_table)\n",
    "        if not os.path.isfile(qt_file):\n",
    "            counter += 1\n",
    "            in_file = os.path.join(in_dir, query_table)\n",
    "            shutil.copy(in_file, query_folder)\n",
    "        if not os.path.isfile(dlt_file):\n",
    "            in_file = os.path.join(in_dir, datalake_table)\n",
    "            shutil.copy(in_file, datalake_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe6637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_dir = \"../data/ugen_v2/data/\"\n",
    "out_dir = \"../data/ugen_v2/\"\n",
    "gtfile = \"../data/ugen_v2/groundtruth.csv\"\n",
    "#create_datalake_and_query_folders(in_dir, out_dir, gtfile, query_col=\"query_table\", datalake_col=\"data_lake_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0874edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_pickle(gtfile, out_pickle, query_col=\"query_table\", datalake_col=\"data_lake_table\", label=\"unionable\"):\n",
    "    query_datalake_dict = {}\n",
    "    gt_df = pd.read_csv(gtfile)\n",
    "    for _, row in gt_df.iterrows():\n",
    "        query_table = row[query_col]\n",
    "        datalake_table = row[datalake_col]\n",
    "        is_unionable = row[label]\n",
    "        if (is_unionable == 1):\n",
    "            if query_table not in query_datalake_dict:\n",
    "                query_datalake_dict[query_table] = [datalake_table]\n",
    "            else:\n",
    "                curr_tables = set(query_datalake_dict[query_table])\n",
    "                curr_tables.add(datalake_table)\n",
    "                query_datalake_dict[query_table] = list(curr_tables)\n",
    "    for key, value in query_datalake_dict.items():\n",
    "        print(key, value)\n",
    "    with open(out_pickle, 'wb') as handle:\n",
    "        pickle.dump(query_datalake_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a31da6-fe26-4374-8c8b-52f223636621",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfile = \"../data/ugen_v2/groundtruth.csv\"\n",
    "out_pickle = \"../data/ugen_v2/groundtruth.pickle\"\n",
    "#create_gt_pickle(gtfile, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309bd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfile= \"../ugen_v1_sparse_20/groundtruth.csv\"\n",
    "out_pickle = \"../ugen_v1_sparse_20/groundtruth.pickle\"\n",
    "#create_datalake_and_query_folders(in_dir,out_dir,gtfile)\n",
    "#create_gt_pickle(gtfile, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d18775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_file(gen_file):\n",
    "    if \"ugen_v2\" in gen_file:\n",
    "        return pd.read_csv(gen_file, sep=';')\n",
    "    data = []\n",
    "    try:\n",
    "        data = pd.read_csv(gen_file, lineterminator='\\n', low_memory=False)\n",
    "        if data.shape[1] < 2:\n",
    "            data = pd.read_csv(gen_file, sep='|')\n",
    "    except:\n",
    "        try:\n",
    "            data = pd.read_csv(gen_file, sep='|')\n",
    "        except:\n",
    "            with open(gen_file) as curr_csv:\n",
    "                curr_data = curr_csv.read().splitlines()\n",
    "                curr_data = [len(row.split('|')) for row in curr_data]\n",
    "                max_col_num = 0\n",
    "                if len(curr_data) != 0:\n",
    "                    max_col_num = max(curr_data)\n",
    "                try:\n",
    "                    if max_col_num != 0:\n",
    "                        df = pd.read_csv(gen_file, sep='|', header=None, names=range(max_col_num), low_memory=False)\n",
    "                        data = df\n",
    "                        return data\n",
    "                    else:\n",
    "                        df = pd.read_csv(gen_file, lineterminator='\\n', low_memory=False)\n",
    "                        data = df\n",
    "                        return data\n",
    "                except:\n",
    "                    df = pd.read_csv(gen_file, lineterminator='\\n', low_memory=False)\n",
    "                    data = df\n",
    "                    return data\n",
    "    return data\n",
    "\n",
    "in_dir = \"../ugen_v1/datalake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75051188",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"../labeled_benchmark\"\n",
    "in_query = in_dir + \"/query\"\n",
    "in_datalake = in_dir + \"/datalake\"\n",
    "in_gpt_dir = \"../data/ugen_v2\"\n",
    "in_gpt_query = in_gpt_dir + \"/query\"\n",
    "in_gpt_datalake = in_gpt_dir + \"/datalake\"\n",
    "\n",
    "def find_avg_shape(in_dir):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    csv_files = [file for file in os.listdir(in_dir)]\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        rows.append(df.shape[0])\n",
    "        cols.append(df.shape[1])\n",
    "    return sum(rows)/len(rows), sum(cols)/len(rows)\n",
    "print(find_avg_shape(in_gpt_query))\n",
    "print(find_avg_shape(in_gpt_datalake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgLength(feature_vector, column, mode):\n",
    "    column_values= copy.copy(feature_vector[column])\n",
    "    column_values.fillna('nan')\n",
    "    lengths = []       \n",
    "    for i in column_values.values:\n",
    "        if i!='nan': \n",
    "            if mode == 'tokens' : lengths.append(len(str(i)) )\n",
    "            elif mode == 'words' : lengths.append(len(re.split(';|,|_|\\|', str(i))))\n",
    "\n",
    "    avg = 0 if len(lengths) == 0 else round(float(sum(lengths) / len(lengths)),2)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    words = re.split(';|,|_|\\|', str(text))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_len_all(in_dir):\n",
    "    short_string_counter = 0\n",
    "    medium_string_counter = 0\n",
    "    long_string_counter = 0\n",
    "    csv_files = [file for file in os.listdir(in_dir) if file.endswith('.csv')]\n",
    "    total_column_counter = 0\n",
    "    for file in csv_files:\n",
    "        average_lengths = {}\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        for column in df.columns:\n",
    "            total_column_counter += 1\n",
    "            if df[column].dtype == 'object':  # Check if column contains text data\n",
    "                df[str(column) + '_Word_Length'] = df[column].apply(lambda x: np.mean([len(word) for word in get_words(x)]))\n",
    "                average_lengths[column] = df[str(column) + '_Word_Length'].mean()\n",
    "        for key, value in average_lengths.items():\n",
    "            if value >= 6:\n",
    "                long_string_counter += 1\n",
    "            elif value >= 3 and value < 6:\n",
    "                medium_string_counter += 1 \n",
    "            else:\n",
    "                short_string_counter += 1\n",
    "    total_column_counter = float(total_column_counter)\n",
    "    return short_string_counter*100/total_column_counter, medium_string_counter*100/total_column_counter, long_string_counter*100/total_column_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(in_dir):\n",
    "    total_numeric_columns = 0\n",
    "    csv_files = [file for file in os.listdir(in_dir) if file.endswith('.csv')]\n",
    "    total_column_counter = 0\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "        non_null_numeric_columns = df[numeric_columns].notnull().any().sum()\n",
    "        total_numeric_columns += non_null_numeric_columns\n",
    "        total_column_counter += len(df.columns)\n",
    "        #     if df[column].dtype != 'object' and not(df[column].isnull().all()):  # Check if column contains text data\n",
    "        #         num_column_counter += 1\n",
    "    return total_numeric_columns*100/float(total_column_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_densities(in_dir):\n",
    "    densities = []\n",
    "    csv_files = [file for file in os.listdir(in_dir) if file.endswith('.csv')]\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        column_densities = df.notnull().mean()\n",
    "        densities.append(column_densities)\n",
    "    concatenated_densities = pd.concat(densities, axis=1)\n",
    "    average_density = concatenated_densities.mean().mean()\n",
    "    return average_density\n",
    "\n",
    "def get_all_nulls(in_dir):\n",
    "    densities = []\n",
    "    csv_files = [file for file in os.listdir(in_dir) if file.endswith('.csv')]\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        column_densities = df.isnull().sum()\n",
    "        densities.append(column_densities)\n",
    "    concatenated_densities = pd.concat(densities, axis=1)\n",
    "    all_nulls = concatenated_densities.sum().sum()\n",
    "    return all_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96397032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_count(in_dir):\n",
    "    less_than_20 = 0\n",
    "    between_20_50 = 0\n",
    "    greater_than_50 = 0\n",
    "    csv_files = [file for file in os.listdir(in_dir) if file.endswith('.csv')]\n",
    "    total_column_counter = 0\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(in_dir, file)\n",
    "        df = read_file(file_path)\n",
    "        total_rows = len(df)\n",
    "        total_column_counter += len(df.columns)\n",
    "        unique_value_percentages = df.nunique() / total_rows * 100\n",
    "        for i in unique_value_percentages:\n",
    "            if i < 20.0:\n",
    "                less_than_20 += 1\n",
    "            elif i > 50.0:\n",
    "                greater_than_50 += 1\n",
    "            else:\n",
    "                between_20_50 += 1\n",
    "    total_column_counter = float(total_column_counter)\n",
    "    return (less_than_20*100/total_column_counter, between_20_50*100/total_column_counter, greater_than_50*100/total_column_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_table_num(in_dir):\n",
    "    file_count = 0\n",
    "    for root, _, files in os.walk(in_dir):\n",
    "        if root == in_dir:\n",
    "            continue\n",
    "        file_count += len(files)\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"../data/ugen_v2\"\n",
    "in_query = in_dir + \"/query\"\n",
    "in_datalake = in_dir + \"/datalake\"\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Total Number of Dataset Tables\", get_total_table_num(in_dir))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average Shape in Query\", find_avg_shape(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average Shape in Datalake\", find_avg_shape(in_datalake))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Query avg length\", get_avg_len_all(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Datalake avg length\", get_avg_len_all(in_datalake))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"number of Number type columns in Query\", get_numbers(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"number of Number type columns in Datalake\", get_numbers(in_datalake))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"average density in Query\", get_densities(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"average density in Datalake\", get_densities(in_datalake))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"uniqueness Query\", get_unique_count(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"uniqueness Datalake\", get_unique_count(in_datalake))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"total nulls in Query\", get_all_nulls(in_query))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"total nulls in Datalake\", get_all_nulls(in_datalake))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fe942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANTOS has no labeled not-unionable\n",
    "in_dir = \"../../starmie/data/santos-large\"\n",
    "gt = in_dir + \"/santos_large_benchmark_groundtruth.csv\"\n",
    "gt_df = pd.read_csv(gt)\n",
    "print(len(gt_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUS\n",
    "tus_gt = \"../data/table-union-search-benchmark/small/tus-small-gt/recall_groundtruth.csv\"\n",
    "tus_gt_df = pd.read_csv(tus_gt)\n",
    "\n",
    "# Calculate the sum of the numbers in the specified column\n",
    "sum_of_column = tus_gt_df['unionable_count'].sum()\n",
    "\n",
    "print(\"total labeled unionable pairs:\", sum_of_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
