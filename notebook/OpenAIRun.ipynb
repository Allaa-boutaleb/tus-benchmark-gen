{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deWmOJecfbBr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nDUlDN-soGG",
    "outputId": "e2cf2b42-62ac-40ad-c8fa-dda37bbc2705"
   },
   "outputs": [],
   "source": [
    "root_dir = \"\"\n",
    "data_dir = \"../data/\"\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=False)\n",
    "    root_dir = \"/content/gdrive/My Drive/gen/\"\n",
    "    data_dir = root_dir + \"data/\"\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhQSDoYe0ly4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from csv import writer\n",
    "import random\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003', temperature=1, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY7AHGgaRSc_"
   },
   "outputs": [],
   "source": [
    "def gen_topics(num_topics=50, max_runs=2, save_output_file_path=None):\n",
    "    curr_run = 0\n",
    "    generate_topics = f\"Generate {num_topics} topics useful for inter-table tasks.\"\n",
    "    topics = davinci(generate_topics)\n",
    "    curr_run += 1\n",
    "    post_process_topics = re.split('\\d+.', topics.replace(\"\\n\", \" \"))\n",
    "    final_topics = []\n",
    "    for topic in post_process_topics:\n",
    "        topic = topic.strip()\n",
    "        if topic:\n",
    "            final_topics.append(topic)\n",
    "    while len(set(final_topics)) <= num_topics:\n",
    "        required_num = num_topics - len(set(final_topics))\n",
    "        previous_topics = \",\".join(final_topics)\n",
    "        if curr_run < max_runs:\n",
    "            generate_new_topics = f\"Here are a set of topics: {previous_topics}. Generate {required_num} topics useful for inter-table tasks that are different from the previous topics.\"\n",
    "            newer_topics = davinci(generate_new_topics)\n",
    "            curr_run += 1\n",
    "            post_process_topics = re.split('\\d+.', newer_topics.replace(\"\\n\", \" \"))\n",
    "            for topic in post_process_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic:\n",
    "                    final_topics.append(topic)\n",
    "                else:\n",
    "                    break\n",
    "    if save_output_file_path is not None:\n",
    "        f = open(save_output_file_path,'w')\n",
    "        f.write(\"Topics\\n\")\n",
    "        for t in final_topics:\n",
    "            f.write(t+\"\\n\")\n",
    "            f.close()\n",
    "    return final_topics[:num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7So_ag3mwjyq"
   },
   "outputs": [],
   "source": [
    "num_topics=50\n",
    "output_file_path = data_dir + f\"topics_{num_topics}.txt\"\n",
    "# if topic file is present, we can just read that txt file\n",
    "topics = []\n",
    "if os.path.isfile(output_file_path):\n",
    "    f = open(output_file_path,'r')\n",
    "    lines = f.readlines()\n",
    "    topics = [t.strip() for t in lines[1:num_topics+1]]\n",
    "else:\n",
    "    topics = gen_topics(save_output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqTPVB98RhzW",
    "outputId": "25f96a52-af34-4242-ee33-c9f71cc89121"
   },
   "outputs": [],
   "source": [
    "print(\"Topics\", topics)\n",
    "print(\"Topics len\", len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVSsC3iGEPAp"
   },
   "outputs": [],
   "source": [
    "from numpy.core.multiarray import empty\n",
    "import random\n",
    "import string\n",
    "\n",
    "def ran_gen(size, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for x in range(size))\n",
    "\n",
    "\n",
    "def gen_table(task, task_label, table_1, table_2, table_separation=\"pipe\", save_output_dir=None, save_ground_truth=None):\n",
    "    is_success_task = \"cannot\" if not task_label else \"can\"\n",
    "    separator_char = \",\"\n",
    "    if table_separation == \"pipe\":\n",
    "        separator_char = \"|\"\n",
    "    prompt = f\"Create 2 tables with cells separated by {separator_char}. \"\n",
    "    # Table 1 Characteristics\n",
    "    curr_topic_1 = table_1['topic']\n",
    "    curr_rows_1 = table_1['rows']\n",
    "    curr_columns_1 = table_1['columns']\n",
    "    curr_textuality_1 = table_1['textuality'] * curr_rows_1 * curr_columns_1\n",
    "    prompt += f\"Table 1 has {curr_columns_1} columns, {curr_rows_1} rows, and {curr_textuality_1} words, related to {curr_topic_1}. \"\n",
    "    # Table 2 Characteristics\n",
    "    curr_topic_2 = table_2['topic']\n",
    "    curr_rows_2 = table_2['rows']\n",
    "    curr_columns_2 = table_2['columns']\n",
    "    curr_textuality_2 = table_2['textuality'] * curr_rows_2 * curr_columns_2\n",
    "    prompt += f\"Table 2 has {curr_columns_2} columns, {curr_rows_2} rows, and {curr_textuality_2} words, related to {curr_topic_2}. \"\n",
    "    # Topics for both tables\n",
    "    topics = [curr_topic_1, curr_topic_2]\n",
    "    # Set task label in the prompt\n",
    "    if task_label:\n",
    "        match_column_num = random.randint(2, 5)\n",
    "        prompt += f\"They can be {task}ed because they have only {match_column_num} semantically common columns and at least 1 related same row values across the tables. \"\n",
    "    else:\n",
    "        prompt += f\"They cannot be {task}ed because they have 0 semantically common columns. \"\n",
    "    # Instruct format of answering the task\n",
    "    prompt += \"Answer the above task in the following format:\\n\"\n",
    "    prompt += \"Table 1: {table 1}\\nTable 2: {table 2}\\n\"\n",
    "    if task_label:\n",
    "        prompt += \"\\nKey: {key column in table 1}\\n\"\n",
    "    if \"table\" in table_1:\n",
    "        table_1_table = table_1[\"table\"]\n",
    "        prompt += f\"Table 1: {table_1_table}\"\n",
    "    else:\n",
    "        prompt += \"Table 1:\"\n",
    "    prompt_output = davinci(prompt)\n",
    "    table_inds_to_process = []\n",
    "    if \"table\" in table_1:\n",
    "        if task_label:\n",
    "            match_output = re.search(r\"(?<=Table 2:)(.*)(?=Key)\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        else:\n",
    "            match_output = re.search(r\"(?<=Table 2:)(.*)\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if match_output:\n",
    "            curr_table = match_output.group(0)\n",
    "            lines = curr_table.strip().splitlines()\n",
    "            cleaned_table = \"\\n\".join([line for line in lines if '|' in line])\n",
    "            table_2['table'] = cleaned_table\n",
    "            table_inds_to_process.append(1)\n",
    "        else:\n",
    "            return prompt, prompt_output, None, None, None\n",
    "    else:\n",
    "        match_output = re.search(r\"(.*)(?=Table 2)\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if match_output:\n",
    "            curr_table = match_output.group(0)\n",
    "            lines = curr_table.strip().splitlines()\n",
    "            cleaned_table = \"\\n\".join([line for line in lines if '|' in line])\n",
    "            table_1['table'] = cleaned_table\n",
    "            table_inds_to_process.append(0)\n",
    "        else:\n",
    "            return prompt, prompt_output, None, None, None\n",
    "        if task_label:\n",
    "            match_output = re.search(r\"(?<=Table 2:)(.*)(?=Key)\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        else:\n",
    "            match_output = re.search(r\"(?<=Table 2:).*\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if match_output:\n",
    "            curr_table = match_output.group(0)\n",
    "            lines = curr_table.strip().splitlines()\n",
    "            cleaned_table = \"\\n\".join([line for line in lines if '|' in line])\n",
    "            table_2['table'] = cleaned_table\n",
    "            table_inds_to_process.append(1)\n",
    "        else:\n",
    "            return prompt, prompt_output, None, None, None\n",
    "  \n",
    "    # Get groundtruth\n",
    "    table_groundtruth = \"\"\n",
    "    actual_table_index = 0\n",
    "    if task_label:\n",
    "        match_output = re.search(r\"(?<=Key:).*\\n?\", prompt_output, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if match_output:\n",
    "            table_groundtruth = match_output.group(0)\n",
    "        else:\n",
    "            table_groundtruth = \"N/A\"\n",
    "    for t in table_inds_to_process:\n",
    "        curr_table = None\n",
    "        if t == 0:\n",
    "            curr_table = table_1\n",
    "        elif t == 1:\n",
    "            curr_table = table_2\n",
    "        table_rows = curr_table['table'].split(\"\\n\")\n",
    "        if save_output_dir is not None:\n",
    "            if 'table_csv' in curr_table:\n",
    "                continue\n",
    "            ran_gen_id = ran_gen(8)\n",
    "            output_file_gen = f\"{topics[t]}_{ran_gen_id}.csv\"\n",
    "            output_file_path = os.path.join(save_output_dir, output_file_gen)\n",
    "            if t == 0:\n",
    "                table_1['table_csv'] = output_file_gen\n",
    "            elif t == 1:\n",
    "                table_2['table_csv'] = output_file_gen\n",
    "            with open(output_file_path, 'a+') as csv_f:\n",
    "                writer_csv_f = writer(csv_f, delimiter='|')\n",
    "                for t_rows in range(0, len(table_rows)):\n",
    "                    if len(table_rows[t_rows]) >= 2:\n",
    "                        writer_csv_f.writerow(table_rows[t_rows].split('|'))\n",
    "    if save_ground_truth is not None:\n",
    "        with open(save_ground_truth, 'a+') as gtf:\n",
    "            writer_gtf = writer(gtf)\n",
    "            task_gt_label = 0\n",
    "            if task_label:\n",
    "                task_gt_label = 1\n",
    "            writer_gtf.writerow([table_1['table_csv'], table_2['table_csv'], task_gt_label, table_groundtruth])\n",
    "    return prompt, prompt_output, table_1, table_2, table_groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuKp7BICxaCO"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "def generate_sparse_table(input_dir, table_filepath, sparsity, columns_subset=None):   \n",
    "    input_csv = input_dir + \"/\" + table_filepath\n",
    "    if not os.path.exists(input_csv):\n",
    "        print(\"Could not find file\")\n",
    "        return\n",
    "    with open(input_csv, 'r') as input_csv_file:\n",
    "        input_table = []\n",
    "        csv_reader = csv.reader(input_csv_file, delimiter='|')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            input_table.append(row)\n",
    "        if columns_subset == None:\n",
    "            num_columns = len(input_table[0]) - 1\n",
    "            columns_subset = [i for i in range(num_columns)]\n",
    "        num_rows = len(input_table) - 1\n",
    "        num_null_cells = int(sparsity * num_rows * len(columns_subset))\n",
    "        # Generate a list of all cell positions in the table\n",
    "        all_positions = []\n",
    "        for i in range(num_rows):\n",
    "            for j in range(len(columns_subset)):\n",
    "                curr_row = input_table[i]\n",
    "                if 0 <= j < len(curr_row):\n",
    "                    all_positions.append((i,j))\n",
    "        # Randomly select positions to set as null (None) values\n",
    "        null_positions = random.sample(all_positions, num_null_cells)\n",
    "        # Create the table with null values\n",
    "        for i, row in enumerate(input_table):\n",
    "            for j, value in enumerate(row):\n",
    "                if (i, j) in null_positions:\n",
    "                    input_table[i][j] = None\n",
    "        #table = [[None if (i, j) in null_positions else input_table[i][j] for j in range(len(columns_subset))] for i in range(num_rows)]\n",
    "        ran_gen_id = ran_gen(8)\n",
    "        table_initial = table_filepath.split('.')[0]\n",
    "        output_file_gen = f\"{table_initial}_{ran_gen_id}.csv\"\n",
    "        return input_table, output_file_gen\n",
    "\n",
    "\n",
    "def gen_sparse_union_benchmark(sparsity, benchmark_src_dir, benchmark_tgt_dir, groundtruth_src_csv, groundtruth_tgt_csv,continuation=False):\n",
    "    if groundtruth_tgt_csv is not None:\n",
    "        with open(groundtruth_tgt_csv, 'a+') as gtf:\n",
    "            writer_gtf = writer(gtf)\n",
    "            if not continuation:\n",
    "                writer_gtf.writerow([\"query_table\", \"data_lake_table\", \"unionable\", \"intent_col_name\"])\n",
    "            gtf.flush()\n",
    "    with open(groundtruth_src_csv, 'r') as src_gtf:\n",
    "        csv_reader = csv.reader(src_gtf, delimiter=',')\n",
    "        next(csv_reader)\n",
    "        query_tables = {}\n",
    "        datalake_tables = {}\n",
    "        for row in csv_reader:\n",
    "            query_table, datalake_table, unionable, intent_col = row\n",
    "            \n",
    "            if query_table not in query_tables:\n",
    "                query_sparse_table, query_sparse_table_name = generate_sparse_table(benchmark_src_dir + \"/query\", query_table, sparsity)\n",
    "                query_tables[query_table] = [query_sparse_table, query_sparse_table_name]\n",
    "            if datalake_table not in datalake_tables:\n",
    "                datalake_sparse_table, datalake_sparse_table_name = generate_sparse_table(benchmark_src_dir + \"/datalake\", datalake_table, sparsity)\n",
    "                datalake_tables[datalake_table] = [datalake_sparse_table, datalake_sparse_table_name]\n",
    "                \n",
    "            query_info = query_tables[query_table]\n",
    "            datalake_info = datalake_tables[datalake_table]\n",
    "            \n",
    "            query_sparse_full_path = os.path.join(benchmark_tgt_dir + \"/query\", query_info[1])\n",
    "            datalake_sparse_full_path = os.path.join(benchmark_tgt_dir + \"/datalake\", datalake_info[1])\n",
    "            \n",
    "            if not os.path.exists(query_sparse_full_path):\n",
    "                with open(query_sparse_full_path, 'a+') as csv_f:\n",
    "                    writer_csv_f = writer(csv_f, delimiter='|')\n",
    "                    curr_table = query_info[0]\n",
    "                    for t_rows in range(0, len(curr_table)):\n",
    "                        writer_csv_f.writerow(curr_table[t_rows])\n",
    "\n",
    "            if not os.path.exists(datalake_sparse_full_path):\n",
    "                with open(datalake_sparse_full_path, 'a+') as csv_f:\n",
    "                    writer_csv_f = writer(csv_f, delimiter='|')\n",
    "                    curr_table = datalake_info[0]\n",
    "                    for t_rows in range(0, len(curr_table)):\n",
    "                        writer_csv_f.writerow(curr_table[t_rows])\n",
    "            \n",
    "            with open(groundtruth_tgt_csv, 'a+') as gtf:\n",
    "                writer_gtf = writer(gtf)\n",
    "                writer_gtf.writerow([query_info[1], datalake_info[1], unionable, intent_col])\n",
    "                \n",
    "    \n",
    "benchmark_src_dir = '../ugen_v1'\n",
    "benchmark_tgt_dir = '../ugen_v1_sparse_20'\n",
    "groundtruth_src_csv = f'{benchmark_src_dir}/groundtruth.csv'\n",
    "groundtruth_tgt_csv = f'{benchmark_tgt_dir}/groundtruth.csv'\n",
    "sparsity=0.20\n",
    "\n",
    "if not os.path.exists(benchmark_tgt_dir):\n",
    "    os.makedirs(benchmark_tgt_dir)\n",
    "    os.makedirs(benchmark_tgt_dir + \"/query\")\n",
    "    os.makedirs(benchmark_tgt_dir + \"/datalake\")\n",
    "     \n",
    "gen_sparse_union_benchmark(sparsity, benchmark_src_dir, benchmark_tgt_dir, groundtruth_src_csv, groundtruth_tgt_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1d1Bt5xGcoe"
   },
   "outputs": [],
   "source": [
    "def reset_table(topic):\n",
    "    table = {}\n",
    "    table['topic'] = topic\n",
    "    table['rows'] = random.randint(5, 20)\n",
    "    table['columns'] = random.randint(10, 15)\n",
    "    table['sparsity'] = random.uniform(0, 1)\n",
    "    table['textuality'] = random.randint(1, 5)\n",
    "    return table\n",
    "\n",
    "def gen_union_benchmark(log_file, benchmark_dir, groundtruth_csv, topics, data_lake_size=12, continuation=False):\n",
    "    with open(data_dir + 'log.txt', 'a+') as f:\n",
    "        if not os.path.exists(benchmark_dir):\n",
    "            os.makedirs(benchmark_dir)\n",
    "        output_dir = benchmark_dir + \"data\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        groundtruth_csv = benchmark_dir + \"groundtruth.csv\"\n",
    "        with open(groundtruth_csv, 'a+') as gtf:\n",
    "            writer_gtf = writer(gtf)\n",
    "            if not continuation:\n",
    "                writer_gtf.writerow([\"query_table\", \"data_lake_table\", \"unionable\", \"intent_col_name\"])\n",
    "            gtf.flush()\n",
    "        curr_topic_counter = 48\n",
    "        curr_range_counter = 0\n",
    "        while curr_topic_counter < len(topics):\n",
    "            print(\"At the first while loop: \", curr_topic_counter)\n",
    "            topic_index = curr_topic_counter\n",
    "            task = \"union\"\n",
    "            task_label = True\n",
    "            topic = topics[topic_index]\n",
    "            print(topic)\n",
    "            table_1 = reset_table(topic)\n",
    "            table_2 = reset_table(topic)\n",
    "            prompt, tables, table_1, table_2_copy, table_groundtruth = gen_table(task, task_label, table_1, table_2,\n",
    "                                                                                 table_separation=\"pipe\",\n",
    "                                                                                 save_output_dir=output_dir,\n",
    "                                                                                 save_ground_truth=groundtruth_csv)\n",
    "            f.write(f\"\\nprompt: {prompt}\\n\")\n",
    "            f.write(f\"\\ntables:\\n\")\n",
    "            f.write(tables)\n",
    "            f.flush()\n",
    "            if table_1 is None:\n",
    "                continue\n",
    "            curr_range_counter = 1\n",
    "            while curr_range_counter <= (data_lake_size - 1):\n",
    "                i = curr_range_counter\n",
    "                if i % 2 == 0:\n",
    "                    task_label = True\n",
    "                else:\n",
    "                    task_label = False\n",
    "                prompt, tables, table_1_copy, table_2_copy, table_groundtruth = gen_table(task, task_label, table_1, reset_table(topic),\n",
    "                                                                                          table_separation=\"pipe\",\n",
    "                                                                                          save_output_dir=output_dir,\n",
    "                                                                                          save_ground_truth=groundtruth_csv)\n",
    "                f.write(f\"\\nprompt: {prompt}\\n\")\n",
    "                f.write(f\"\\ntables:\\n\")\n",
    "                f.write(tables)\n",
    "                f.flush()\n",
    "                if table_1_copy is not None:\n",
    "                    curr_range_counter += 1\n",
    "                    print(\"curr_range_counter: \", curr_range_counter)\n",
    "                if (table_1 is not None) and (curr_range_counter == data_lake_size):\n",
    "                    curr_topic_counter += 1\n",
    "                    curr_range_counter = 0\n",
    "                    print(\"curr_topic_counter: \", curr_topic_counter)\n",
    "                    break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C-XLGZvyoCgi",
    "outputId": "8620dda8-ea3e-4fd2-d171-3063b39fcd7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_file = data_dir + 'log.txt'\n",
    "benchmark_dir = root_dir + \"ugen_v1/\"\n",
    "os.makedirs(benchmark_dir, exist_ok=True)\n",
    "groundtruth_csv = benchmark_dir + \"groundtruth.csv\"\n",
    "gen_union_benchmark(log_file, benchmark_dir, groundtruth_csv, topics, data_lake_size=20, continuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
