{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7538d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pickle5 as p\n",
    "import pandas as pd\n",
    "from matplotlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e17562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDictionaryFromPickleFile(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = p.load(filePointer)\n",
    "    filePointer.close()\n",
    "    return dictionary\n",
    "\n",
    "def loadDictionaryFromPickleFileList(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = p.load(filePointer)\n",
    "    filePointer.close()\n",
    "    actual_dict = {}\n",
    "    for curr_dict in dictionary:\n",
    "        actual_dict[curr_dict['query_table']] = curr_dict['result_set']\n",
    "    return actual_dict\n",
    "\n",
    "def saveDictionaryAsPickleFile(dictionary, dictionaryPath):\n",
    "    ''' Save dictionary as a pickle file\n",
    "    Args:\n",
    "        dictionary to be saved\n",
    "        dictionaryPath: filepath to which the dictionary will be saved\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'wb')\n",
    "    pickle.dump(dictionary,filePointer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    filePointer.close()\n",
    "\n",
    "\n",
    "def calcMetrics(max_k, k_range, gtPath=None, resPath=None, record=True):\n",
    "    ''' Calculate and log the performance metrics: MAP, Precision@k, Recall@k\n",
    "    Args:\n",
    "        max_k: the maximum K value (e.g. for SANTOS benchmark, max_k = 10. For TUS benchmark, max_k = 60)\n",
    "        k_range: step size for the K's up to max_k\n",
    "        gtPath: file path to the groundtruth\n",
    "        resPath: file path to the raw results from the model\n",
    "        record (boolean): to log in MLFlow or not\n",
    "    Return: MAP, P@K, R@K\n",
    "    '''\n",
    "    groundtruth = loadDictionaryFromPickleFile(gtPath)\n",
    "    resultFile = loadDictionaryFromPickleFile(resPath)\n",
    "    fullresultFile = resultFile\n",
    "    if type(resultFile) == list:\n",
    "        resultFile = loadDictionaryFromPickleFileList(resPath)\n",
    "    # =============================================================================\n",
    "    # Precision and recall\n",
    "    # =============================================================================\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    final_results = []\n",
    "    for k in range(1, max_k+1):\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        rec = 0\n",
    "        ideal_recall = []\n",
    "        index_counter = 0\n",
    "        for table in resultFile:\n",
    "            # t28 tables have less than 60 results. So, skipping them in the analysis.\n",
    "            if table.split(\"____\",1)[0] != \"t_28dc8f7610402ea7\": \n",
    "                if table in groundtruth:\n",
    "                    groundtruth_set = set(groundtruth[table])\n",
    "                    groundtruth_set = {x.split(\".\")[0] for x in groundtruth_set}\n",
    "                    result_set = resultFile[table][:k]\n",
    "                    result_set = [x.split(\".\")[0] for x in result_set]\n",
    "                    # find_intersection = true positives\n",
    "                    #if len(resultFile[table]) > 10 and k==10:\n",
    "                    #    print(\"LEN RESULT SET\", len(resultFile[table]))\n",
    "                    #    print(fullresultFile[index_counter]['confidence_set'])\n",
    "                    find_intersection = set(result_set).intersection(groundtruth_set)\n",
    "                    curr_result_dict = {}\n",
    "                    curr_result_dict[\"groundtruth_set\"] = groundtruth_set\n",
    "                    curr_result_dict[\"result_set\"] = result_set\n",
    "                    curr_result_dict[\"intersection\"] = find_intersection\n",
    "                    final_results.append(curr_result_dict)\n",
    "                    tp = len(find_intersection)\n",
    "                    fp = k - tp\n",
    "                    fn = len(groundtruth_set) - tp\n",
    "                    if len(groundtruth_set)>=k: \n",
    "                        true_positive += tp\n",
    "                        false_positive += fp\n",
    "                        false_negative += fn\n",
    "                    rec += tp / (tp+fn)\n",
    "                    ideal_recall.append(k/len(groundtruth[table]))\n",
    "            index_counter += 1\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = rec/len(resultFile)\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "#         if k % 10 == 0:\n",
    "        print(k, \"IDEAL RECALL:\", sum(ideal_recall)/len(ideal_recall))\n",
    "    used_k = [k_range]\n",
    "    if max_k >k_range:\n",
    "        for i in range(k_range * 2, max_k+1, k_range):\n",
    "            used_k.append(i)\n",
    "    print(\"--------------------------\")\n",
    "    for k in used_k:\n",
    "        print(\"Precision at k = \",k,\"=\", precision_array[k-1])\n",
    "        print(\"Recall at k = \",k,\"=\", recall_array[k-1])\n",
    "        print(\"--------------------------\")\n",
    "    \n",
    "    map_sum = 0\n",
    "    for k in range(0, max_k):\n",
    "        map_sum += precision_array[k]\n",
    "    mean_avg_pr = map_sum/max_k\n",
    "    print(\"The mean average precision is:\", mean_avg_pr)\n",
    "    output_result_csv_file = \"curr_run_results.csv\"\n",
    "    pd.DataFrame(final_results).to_csv(output_result_csv_file)\n",
    "\n",
    "    # logging to mlflow\n",
    "    if record: # if the user would like to log to MLFlow\n",
    "        mlflow.log_metric(\"mean_avg_precision\", mean_avg_pr)\n",
    "        mlflow.log_metric(\"prec_k\", precision_array[max_k-1])\n",
    "        mlflow.log_metric(\"recall_k\", recall_array[max_k-1])\n",
    "\n",
    "    return mean_avg_pr, precision_array[max_k-1], recall_array[max_k-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "calcMetrics(max_k=10, k_range=1, \n",
    "            gtPath='../data/ugen_v2/santosUnionBenchmark.pickle', \n",
    "            resPath='../starmie-llm-results/vicuna7b_ugen_v1_sparse_20_icl-1_result.pickle', record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = loadDictionaryFromPickleFile('data/d3l_santos/santos_benchmark_result_by_d3l.pickle')\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datalake_and_query_folders(in_dir, out_dir, gtfile, query_col=\"query_table\", datalake_col=\"data_lake_table\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    query_folder = os.path.join(out_dir, \"query\")\n",
    "    datalake_folder = os.path.join(out_dir, \"datalake\")\n",
    "    os.makedirs(query_folder, exist_ok=True)\n",
    "    os.makedirs(datalake_folder, exist_ok=True)\n",
    "    gt_df = pd.read_csv(gtfile)\n",
    "    counter = 0\n",
    "    for _, row in gt_df.iterrows():\n",
    "        query_table = row[query_col]\n",
    "        datalake_table = row[datalake_col]\n",
    "        qt_file = os.path.join(query_folder, query_table)\n",
    "        dlt_file = os.path.join(datalake_folder, datalake_table)\n",
    "        if not os.path.isfile(qt_file):\n",
    "            counter += 1\n",
    "            in_file = os.path.join(in_dir, query_table)\n",
    "            shutil.copy(in_file, query_folder)\n",
    "#         if not os.path.isfile(dlt_file):\n",
    "#             in_file = os.path.join(in_dir, datalake_table)\n",
    "#             shutil.copy(in_file, datalake_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae18c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data/table-union-search-benchmark/small/santos-query\"\n",
    "out_dir = \"data/table-union-search-benchmark/small\"\n",
    "gtfile = \"TUS_benchmark_relabeled_groundtruth.csv\"\n",
    "create_datalake_and_query_folders(in_dir, out_dir, gtfile, query_col=\"query_table\", datalake_col=\"data_lake_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_pickle(gtfile, out_pickle, query_col=\"query_table\", datalake_col=\"data_lake_table\", label=\"unionable\"):\n",
    "    query_datalake_dict = {}\n",
    "    gt_df = pd.read_csv(gtfile)\n",
    "    for _, row in gt_df.iterrows():\n",
    "        query_table = row[query_col]\n",
    "        datalake_table = row[datalake_col]\n",
    "        is_unionable = row[label]\n",
    "        if (is_unionable == 1):\n",
    "            if query_table not in query_datalake_dict:\n",
    "                query_datalake_dict[query_table] = [datalake_table]\n",
    "            else:\n",
    "                curr_tables = set(query_datalake_dict[query_table])\n",
    "                curr_tables.add(datalake_table)\n",
    "                query_datalake_dict[query_table] = list(curr_tables)\n",
    "    for key, value in query_datalake_dict.items():\n",
    "        print(len(value))\n",
    "        print(key, value)\n",
    "    with open(out_pickle, 'wb') as handle:\n",
    "        pickle.dump(query_datalake_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
